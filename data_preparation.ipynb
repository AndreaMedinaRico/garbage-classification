{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d210b8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ca263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be112597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, random, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0c2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d692974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Versión PyTorch: 2.5.1+cu121\n",
      "Versión CUDA usada: 12.1\n",
      "Dispositivo: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"Versión PyTorch:\", torch.__version__)\n",
    "print(\"Versión CUDA usada:\", torch.version.cuda)\n",
    "print(\"Dispositivo:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad1197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivos detectados por TensorFlow:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "⚠️ TensorFlow está usando solo la CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Dispositivos detectados por TensorFlow:\")\n",
    "print(tf.config.list_physical_devices())\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"✅ TensorFlow está usando la GPU\")\n",
    "else:\n",
    "    print(\"⚠️ TensorFlow está usando solo la CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2c7432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d3a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\andre\\.cache\\kagglehub\\datasets\\robikscube\\textocr-text-extraction-from-images-dataset\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"robikscube/textocr-text-extraction-from-images-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c96520",
   "metadata": {},
   "source": [
    "## Manejo de archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09bebcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpetas dentro:  ['annot.csv', 'annot.parquet', 'img.csv', 'img.parquet', 'sample', 'test', 'TextOCR_0.1_train.json', 'train', 'train_val_images', 'val']\n"
     ]
    }
   ],
   "source": [
    "print(\"Carpetas dentro: \", os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5ebb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>utf8_string</th>\n",
       "      <th>points</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a4ea732cd3d5948a_1</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[525.83, 3.4, 197.64, 33.94]</td>\n",
       "      <td>Performance</td>\n",
       "      <td>[525.83, 3.4, 723.47, 7.29, 722.76, 36.99, 525...</td>\n",
       "      <td>6707.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a4ea732cd3d5948a_2</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[534.67, 64.68, 91.22, 38.19]</td>\n",
       "      <td>Sport</td>\n",
       "      <td>[535.73, 64.68, 623.41, 67.51, 625.89, 102.87,...</td>\n",
       "      <td>3483.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a4ea732cd3d5948a_3</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[626.95, 63.62, 96.52, 31.82]</td>\n",
       "      <td>Watch</td>\n",
       "      <td>[626.95, 63.62, 721.7, 63.62, 723.47, 95.44, 6...</td>\n",
       "      <td>3071.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a4ea732cd3d5948a_4</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[577.4, 141.87, 147.13, 43.1]</td>\n",
       "      <td>...period.</td>\n",
       "      <td>[580.02, 143.61, 724.53, 141.87, 723.66, 184.9...</td>\n",
       "      <td>6341.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a4ea732cd3d5948a_5</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[391.03, 163.9, 60.82, 38.65]</td>\n",
       "      <td>.</td>\n",
       "      <td>[395.2, 163.9, 451.85, 191.94, 445.59, 202.55,...</td>\n",
       "      <td>2350.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  id          image_id  \\\n",
       "0           0  a4ea732cd3d5948a_1  a4ea732cd3d5948a   \n",
       "1           1  a4ea732cd3d5948a_2  a4ea732cd3d5948a   \n",
       "2           2  a4ea732cd3d5948a_3  a4ea732cd3d5948a   \n",
       "3           3  a4ea732cd3d5948a_4  a4ea732cd3d5948a   \n",
       "4           4  a4ea732cd3d5948a_5  a4ea732cd3d5948a   \n",
       "\n",
       "                            bbox  utf8_string  \\\n",
       "0   [525.83, 3.4, 197.64, 33.94]  Performance   \n",
       "1  [534.67, 64.68, 91.22, 38.19]        Sport   \n",
       "2  [626.95, 63.62, 96.52, 31.82]        Watch   \n",
       "3  [577.4, 141.87, 147.13, 43.1]   ...period.   \n",
       "4  [391.03, 163.9, 60.82, 38.65]            .   \n",
       "\n",
       "                                              points     area  \n",
       "0  [525.83, 3.4, 723.47, 7.29, 722.76, 36.99, 525...  6707.90  \n",
       "1  [535.73, 64.68, 623.41, 67.51, 625.89, 102.87,...  3483.69  \n",
       "2  [626.95, 63.62, 721.7, 63.62, 723.47, 95.44, 6...  3071.27  \n",
       "3  [580.02, 143.61, 724.53, 141.87, 723.66, 184.9...  6341.30  \n",
       "4  [395.2, 163.9, 451.85, 191.94, 445.59, 202.55,...  2350.69  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot = pd.read_csv(path + '/annot.csv')\n",
    "annot.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a45f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>set</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>840</td>\n",
       "      <td>1024</td>\n",
       "      <td>train</td>\n",
       "      <td>train/a4ea732cd3d5948a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4bf43a7b2a898044</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>train</td>\n",
       "      <td>train/4bf43a7b2a898044.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1b55b309b0f50d02</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>train</td>\n",
       "      <td>train/1b55b309b0f50d02.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00c359f294f7dcd9</td>\n",
       "      <td>1024</td>\n",
       "      <td>680</td>\n",
       "      <td>train</td>\n",
       "      <td>train/00c359f294f7dcd9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>04b5a37f762b0f51</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>train</td>\n",
       "      <td>train/04b5a37f762b0f51.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  width  height    set  \\\n",
       "0           0  a4ea732cd3d5948a    840    1024  train   \n",
       "1           1  4bf43a7b2a898044   1024     683  train   \n",
       "2           2  1b55b309b0f50d02   1024     683  train   \n",
       "3           3  00c359f294f7dcd9   1024     680  train   \n",
       "4           4  04b5a37f762b0f51    768    1024  train   \n",
       "\n",
       "                    file_name  \n",
       "0  train/a4ea732cd3d5948a.jpg  \n",
       "1  train/4bf43a7b2a898044.jpg  \n",
       "2  train/1b55b309b0f50d02.jpg  \n",
       "3  train/00c359f294f7dcd9.jpg  \n",
       "4  train/04b5a37f762b0f51.jpg  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = pd.read_csv(path + '/img.csv')\n",
    "info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22bb3e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>set</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>840</td>\n",
       "      <td>1024</td>\n",
       "      <td>train</td>\n",
       "      <td>train/a4ea732cd3d5948a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4bf43a7b2a898044</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>train</td>\n",
       "      <td>train/4bf43a7b2a898044.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1b55b309b0f50d02</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>train</td>\n",
       "      <td>train/1b55b309b0f50d02.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00c359f294f7dcd9</td>\n",
       "      <td>1024</td>\n",
       "      <td>680</td>\n",
       "      <td>train</td>\n",
       "      <td>train/00c359f294f7dcd9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>04b5a37f762b0f51</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>train</td>\n",
       "      <td>train/04b5a37f762b0f51.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          image_id  width  height    set  \\\n",
       "0           0  a4ea732cd3d5948a    840    1024  train   \n",
       "1           1  4bf43a7b2a898044   1024     683  train   \n",
       "2           2  1b55b309b0f50d02   1024     683  train   \n",
       "3           3  00c359f294f7dcd9   1024     680  train   \n",
       "4           4  04b5a37f762b0f51    768    1024  train   \n",
       "\n",
       "                    file_name  \n",
       "0  train/a4ea732cd3d5948a.jpg  \n",
       "1  train/4bf43a7b2a898044.jpg  \n",
       "2  train/1b55b309b0f50d02.jpg  \n",
       "3  train/00c359f294f7dcd9.jpg  \n",
       "4  train/04b5a37f762b0f51.jpg  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = info.rename(columns={'id': 'image_id'})\n",
    "info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca8947a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de anotaciones: 21778\n",
      "Cantidad de imágenes: 21778\n"
     ]
    }
   ],
   "source": [
    "info.drop_duplicates(subset=['image_id'], keep='first', inplace=True)\n",
    "annot.drop_duplicates(subset=['image_id'], keep='first', inplace=True)\n",
    "print(\"Cantidad de anotaciones:\", len(annot))\n",
    "print(\"Cantidad de imágenes:\", len(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f710f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de anotaciones: 21778\n",
      "Cantidad de imágenes: 21778\n"
     ]
    }
   ],
   "source": [
    "info.drop_duplicates(subset=['image_id'], keep='first', inplace=True)\n",
    "annot.drop_duplicates(subset=['image_id'], keep='first', inplace=True)\n",
    "print(\"Cantidad de anotaciones:\", len(annot))\n",
    "print(\"Cantidad de imágenes:\", len(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba4bf72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>utf8_string</th>\n",
       "      <th>points</th>\n",
       "      <th>area</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>set</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21773</th>\n",
       "      <td>1052200</td>\n",
       "      <td>d13b7ec72bd5eed5_1</td>\n",
       "      <td>d13b7ec72bd5eed5</td>\n",
       "      <td>[517.02, 355.52, 74.5, 77.5]</td>\n",
       "      <td>FOR</td>\n",
       "      <td>[517.02, 355.52, 591.02, 355.52, 591.52, 432.5...</td>\n",
       "      <td>5773.75</td>\n",
       "      <td>21773</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>train</td>\n",
       "      <td>train/d13b7ec72bd5eed5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21774</th>\n",
       "      <td>1052214</td>\n",
       "      <td>0022933cdceee189_1</td>\n",
       "      <td>0022933cdceee189</td>\n",
       "      <td>[-0.11, 215.6, 43.2, 47.87]</td>\n",
       "      <td>20</td>\n",
       "      <td>[1.06, 215.6, 43.09, 215.6, 43.09, 259.97, -0....</td>\n",
       "      <td>2067.98</td>\n",
       "      <td>21774</td>\n",
       "      <td>1024</td>\n",
       "      <td>554</td>\n",
       "      <td>train</td>\n",
       "      <td>train/0022933cdceee189.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21775</th>\n",
       "      <td>1052266</td>\n",
       "      <td>6029c75e0325d164_1</td>\n",
       "      <td>6029c75e0325d164</td>\n",
       "      <td>[486.62, 71.33, 163.65, 66.23]</td>\n",
       "      <td>ORIOLES</td>\n",
       "      <td>[486.62, 71.65, 650.27, 71.33, 649.95, 137.56,...</td>\n",
       "      <td>10838.54</td>\n",
       "      <td>21775</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>train</td>\n",
       "      <td>train/6029c75e0325d164.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21776</th>\n",
       "      <td>1052335</td>\n",
       "      <td>0ebbecdc46b78d42_1</td>\n",
       "      <td>0ebbecdc46b78d42</td>\n",
       "      <td>[915.45, 15.84, 67.75, 87.95]</td>\n",
       "      <td>A</td>\n",
       "      <td>[915.45, 15.84, 982.89, 15.84, 983.2, 103.79, ...</td>\n",
       "      <td>5958.61</td>\n",
       "      <td>21776</td>\n",
       "      <td>1024</td>\n",
       "      <td>681</td>\n",
       "      <td>train</td>\n",
       "      <td>train/0ebbecdc46b78d42.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21777</th>\n",
       "      <td>1052350</td>\n",
       "      <td>a37e1fb026b80a6d_1</td>\n",
       "      <td>a37e1fb026b80a6d</td>\n",
       "      <td>[331.69, 462.84, 417.31, 201.08]</td>\n",
       "      <td>RÖR</td>\n",
       "      <td>[331.69, 466.97, 749.0, 462.84, 749.0, 659.79,...</td>\n",
       "      <td>83912.69</td>\n",
       "      <td>21777</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>train</td>\n",
       "      <td>train/a37e1fb026b80a6d.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0_x                  id          image_id  \\\n",
       "21773       1052200  d13b7ec72bd5eed5_1  d13b7ec72bd5eed5   \n",
       "21774       1052214  0022933cdceee189_1  0022933cdceee189   \n",
       "21775       1052266  6029c75e0325d164_1  6029c75e0325d164   \n",
       "21776       1052335  0ebbecdc46b78d42_1  0ebbecdc46b78d42   \n",
       "21777       1052350  a37e1fb026b80a6d_1  a37e1fb026b80a6d   \n",
       "\n",
       "                                   bbox utf8_string  \\\n",
       "21773      [517.02, 355.52, 74.5, 77.5]         FOR   \n",
       "21774       [-0.11, 215.6, 43.2, 47.87]          20   \n",
       "21775    [486.62, 71.33, 163.65, 66.23]     ORIOLES   \n",
       "21776     [915.45, 15.84, 67.75, 87.95]           A   \n",
       "21777  [331.69, 462.84, 417.31, 201.08]         RÖR   \n",
       "\n",
       "                                                  points      area  \\\n",
       "21773  [517.02, 355.52, 591.02, 355.52, 591.52, 432.5...   5773.75   \n",
       "21774  [1.06, 215.6, 43.09, 215.6, 43.09, 259.97, -0....   2067.98   \n",
       "21775  [486.62, 71.65, 650.27, 71.33, 649.95, 137.56,...  10838.54   \n",
       "21776  [915.45, 15.84, 982.89, 15.84, 983.2, 103.79, ...   5958.61   \n",
       "21777  [331.69, 466.97, 749.0, 462.84, 749.0, 659.79,...  83912.69   \n",
       "\n",
       "       Unnamed: 0_y  width  height    set                   file_name  \n",
       "21773         21773   1024     683  train  train/d13b7ec72bd5eed5.jpg  \n",
       "21774         21774   1024     554  train  train/0022933cdceee189.jpg  \n",
       "21775         21775   1024     768  train  train/6029c75e0325d164.jpg  \n",
       "21776         21776   1024     681  train  train/0ebbecdc46b78d42.jpg  \n",
       "21777         21777   1024     683  train  train/a37e1fb026b80a6d.jpg  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_info = pd.merge(annot, info, how = 'inner', on = 'image_id')\n",
    "img_info.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d62e710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de anotaciones e imágenes combinadas: 21778\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de anotaciones e imágenes combinadas:\", len(img_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af94a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>utf8_string</th>\n",
       "      <th>points</th>\n",
       "      <th>area</th>\n",
       "      <th>img_num</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[525.83, 3.4, 197.64, 33.94]</td>\n",
       "      <td>Performance</td>\n",
       "      <td>[525.83, 3.4, 723.47, 7.29, 722.76, 36.99, 525...</td>\n",
       "      <td>6707.90</td>\n",
       "      <td>0</td>\n",
       "      <td>840</td>\n",
       "      <td>1024</td>\n",
       "      <td>a4ea732cd3d5948a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4bf43a7b2a898044</td>\n",
       "      <td>[429.44, 163.75, 27.52, 13.0]</td>\n",
       "      <td>400</td>\n",
       "      <td>[429.44, 163.75, 456.74, 163.75, 456.96, 176.5...</td>\n",
       "      <td>357.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>4bf43a7b2a898044.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b55b309b0f50d02</td>\n",
       "      <td>[564.0, 328.58, 103.5, 41.15]</td>\n",
       "      <td>CAOL</td>\n",
       "      <td>[564.96, 328.58, 667.5, 338.55, 667.18, 369.73...</td>\n",
       "      <td>4259.02</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>1b55b309b0f50d02.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c359f294f7dcd9</td>\n",
       "      <td>[322.37, 368.64, 118.63, 51.07]</td>\n",
       "      <td>G-ATCO</td>\n",
       "      <td>[322.37, 391.39, 436.35, 368.64, 441.0, 400.44...</td>\n",
       "      <td>6058.43</td>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>680</td>\n",
       "      <td>00c359f294f7dcd9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04b5a37f762b0f51</td>\n",
       "      <td>[217.83, 302.27, 98.68, 45.38]</td>\n",
       "      <td>OUR</td>\n",
       "      <td>[218.66, 302.27, 316.51, 303.52, 315.68, 347.6...</td>\n",
       "      <td>4478.10</td>\n",
       "      <td>4</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>04b5a37f762b0f51.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_id                             bbox  utf8_string  \\\n",
       "0  a4ea732cd3d5948a     [525.83, 3.4, 197.64, 33.94]  Performance   \n",
       "1  4bf43a7b2a898044    [429.44, 163.75, 27.52, 13.0]          400   \n",
       "2  1b55b309b0f50d02    [564.0, 328.58, 103.5, 41.15]         CAOL   \n",
       "3  00c359f294f7dcd9  [322.37, 368.64, 118.63, 51.07]       G-ATCO   \n",
       "4  04b5a37f762b0f51   [217.83, 302.27, 98.68, 45.38]          OUR   \n",
       "\n",
       "                                              points     area  img_num  width  \\\n",
       "0  [525.83, 3.4, 723.47, 7.29, 722.76, 36.99, 525...  6707.90        0    840   \n",
       "1  [429.44, 163.75, 456.74, 163.75, 456.96, 176.5...   357.76        1   1024   \n",
       "2  [564.96, 328.58, 667.5, 338.55, 667.18, 369.73...  4259.02        2   1024   \n",
       "3  [322.37, 391.39, 436.35, 368.64, 441.0, 400.44...  6058.43        3   1024   \n",
       "4  [218.66, 302.27, 316.51, 303.52, 315.68, 347.6...  4478.10        4    768   \n",
       "\n",
       "   height             file_name  \n",
       "0    1024  a4ea732cd3d5948a.jpg  \n",
       "1     683  4bf43a7b2a898044.jpg  \n",
       "2     683  1b55b309b0f50d02.jpg  \n",
       "3     680  00c359f294f7dcd9.jpg  \n",
       "4    1024  04b5a37f762b0f51.jpg  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_info = img_info.drop(columns = ['Unnamed: 0_x', 'id', 'set'])\n",
    "img_info = img_info.rename(columns={'Unnamed: 0_y': 'img_num'})\n",
    "img_info['file_name'] = img_info['file_name'].str.replace('train/', '')\n",
    "img_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b8d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21778 entries, 0 to 21777\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   image_id     21778 non-null  object \n",
      " 1   bbox         21778 non-null  object \n",
      " 2   utf8_string  21774 non-null  object \n",
      " 3   points       21778 non-null  object \n",
      " 4   area         21778 non-null  float64\n",
      " 5   img_num      21778 non-null  int64  \n",
      " 6   width        21778 non-null  int64  \n",
      " 7   height       21778 non-null  int64  \n",
      " 8   file_name    21778 non-null  object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "img_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f02d5263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>img_num</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21778.000000</td>\n",
       "      <td>21778.000000</td>\n",
       "      <td>21778.000000</td>\n",
       "      <td>21778.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11161.740411</td>\n",
       "      <td>10888.500000</td>\n",
       "      <td>948.640096</td>\n",
       "      <td>817.007439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>29321.633196</td>\n",
       "      <td>6286.911417</td>\n",
       "      <td>152.755971</td>\n",
       "      <td>163.140660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>257.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>832.292500</td>\n",
       "      <td>5444.250000</td>\n",
       "      <td>956.250000</td>\n",
       "      <td>683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2610.220000</td>\n",
       "      <td>10888.500000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8802.987500</td>\n",
       "      <td>16332.750000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>808531.050000</td>\n",
       "      <td>21777.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>4608.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                area       img_num         width        height\n",
       "count   21778.000000  21778.000000  21778.000000  21778.000000\n",
       "mean    11161.740411  10888.500000    948.640096    817.007439\n",
       "std     29321.633196   6286.911417    152.755971    163.140660\n",
       "min        14.560000      0.000000    259.000000    257.000000\n",
       "25%       832.292500   5444.250000    956.250000    683.000000\n",
       "50%      2610.220000  10888.500000   1024.000000    768.000000\n",
       "75%      8802.987500  16332.750000   1024.000000   1024.000000\n",
       "max    808531.050000  21777.000000   6000.000000   4608.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_info.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39025b71",
   "metadata": {},
   "source": [
    "## Lectura de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90a93e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de imágenes muestreadas: 500\n"
     ]
    }
   ],
   "source": [
    "imgs_path = path + '/train_val_images/train_images'\n",
    "all_images = [os.path.join(imgs_path, f) for f in os.listdir(imgs_path)]\n",
    "random.seed(42)\n",
    "sample_images = random.sample(all_images, 500)\n",
    "print(\"Cantidad de imágenes muestreadas:\", len(sample_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a5fff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.path.join(path, \"sample\")\n",
    "if not os.path.exists(sample_path):\n",
    "    os.makedirs(sample_path)\n",
    "\n",
    "for img in sample_images:\n",
    "    shutil.copy(img, sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "373740a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de archivos muestreados: ['cc63b113d10a3d31.jpg', '0f0ca36db877fc6b.jpg', '014afe1edd4ca5d8.jpg', 'f777d5a22e3b367e.jpg', '412b31ce905437ea.jpg']\n",
      "Cantidad de registros en img_info para las imágenes muestreadas: 446\n"
     ]
    }
   ],
   "source": [
    "sample_imgs_ids = [os.path.basename(f) for f in sample_images]\n",
    "print(\"Nombres de archivos muestreados:\", sample_imgs_ids[:5])\n",
    "\n",
    "img_info_sampled = img_info[img_info['file_name'].isin(sample_imgs_ids)]\n",
    "print(\"Cantidad de registros en img_info para las imágenes muestreadas:\", len(img_info_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d5bfa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad final de imágenes muestreadas: 450\n",
      "Cantidad final de imágenes en la carpeta sample: 450\n"
     ]
    }
   ],
   "source": [
    "for f in sample_imgs_ids:\n",
    "    if f not in img_info_sampled['file_name'].values:\n",
    "        # Eliminar imagen de la carpeta\n",
    "        os.remove(os.path.join(sample_path, f))\n",
    "        # Eliminar del sample_imgs_ids\n",
    "        sample_imgs_ids.remove(f)\n",
    "\n",
    "print(\"Cantidad final de imágenes muestreadas:\", len(sample_imgs_ids))\n",
    "\n",
    "sample_images = [os.path.join(sample_path, f) for f in os.listdir(sample_path)]\n",
    "print(\"Cantidad final de imágenes en la carpeta sample:\", len(sample_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eab48dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de imágenes en train: 308\n",
      "Cantidad de imágenes en val: 146\n",
      "Cantidad de imágenes en test: 147\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(path, \"train\")\n",
    "val_path = os.path.join(path, \"val\")\n",
    "test_path = os.path.join(path, \"test\")\n",
    "\n",
    "for dir in [train_path, val_path, test_path]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "train_samp, temp_samp = train_test_split(sample_images, test_size=0.4, random_state=42)\n",
    "val_samp, test_samp = train_test_split(temp_samp, test_size=0.5, random_state=42)\n",
    "\n",
    "for img in train_samp:\n",
    "    shutil.copy(img, train_path)\n",
    "\n",
    "for img in val_samp:\n",
    "    shutil.copy(img, val_path)\n",
    "\n",
    "for img in test_samp:\n",
    "    shutil.copy(img, test_path)\n",
    "    \n",
    "print(\"Cantidad de imágenes en train:\", len(os.listdir(train_path)))\n",
    "print(\"Cantidad de imágenes en val:\", len(os.listdir(val_path)))\n",
    "print(\"Cantidad de imágenes en test:\", len(os.listdir(test_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75741761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros en img_info para train: 269\n",
      "Cantidad de registros en img_info para val: 90\n",
      "Cantidad de registros en img_info para test: 87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>utf8_string</th>\n",
       "      <th>points</th>\n",
       "      <th>area</th>\n",
       "      <th>img_num</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a183473b9b4bc8f6</td>\n",
       "      <td>[166.08, 0.7, 32.97, 326.88]</td>\n",
       "      <td>LIDER/MARCELLIONO</td>\n",
       "      <td>[199.05, 0.7, 194.75, 326.86, 166.08, 327.58, ...</td>\n",
       "      <td>10777.23</td>\n",
       "      <td>9</td>\n",
       "      <td>357</td>\n",
       "      <td>1024</td>\n",
       "      <td>a183473b9b4bc8f6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>42113d4ea98235b5</td>\n",
       "      <td>[570.17, 204.36, 90.96, 30.32]</td>\n",
       "      <td>McGINLEY</td>\n",
       "      <td>[572.88, 204.36, 661.13, 213.72, 657.18, 234.6...</td>\n",
       "      <td>2757.91</td>\n",
       "      <td>151</td>\n",
       "      <td>1024</td>\n",
       "      <td>683</td>\n",
       "      <td>42113d4ea98235b5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>2efeea2e77a59e8f</td>\n",
       "      <td>[210.27, 127.41, 42.01, 49.73]</td>\n",
       "      <td>9</td>\n",
       "      <td>[210.27, 128.27, 248.85, 127.41, 252.28, 175.4...</td>\n",
       "      <td>2089.16</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>1024</td>\n",
       "      <td>2efeea2e77a59e8f.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>b47636280fde8aee</td>\n",
       "      <td>[691.76, 627.63, 17.56, 14.87]</td>\n",
       "      <td>10</td>\n",
       "      <td>[692.25, 627.63, 708.83, 627.87, 709.32, 642.5...</td>\n",
       "      <td>261.12</td>\n",
       "      <td>339</td>\n",
       "      <td>764</td>\n",
       "      <td>1024</td>\n",
       "      <td>b47636280fde8aee.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>03031fa84a2806a3</td>\n",
       "      <td>[594.47, 89.11, 105.86, 130.54]</td>\n",
       "      <td>L</td>\n",
       "      <td>[594.47, 98.67, 659.34, 89.11, 700.33, 203.73,...</td>\n",
       "      <td>13818.96</td>\n",
       "      <td>362</td>\n",
       "      <td>891</td>\n",
       "      <td>1024</td>\n",
       "      <td>03031fa84a2806a3.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id                             bbox        utf8_string  \\\n",
       "9    a183473b9b4bc8f6     [166.08, 0.7, 32.97, 326.88]  LIDER/MARCELLIONO   \n",
       "151  42113d4ea98235b5   [570.17, 204.36, 90.96, 30.32]           McGINLEY   \n",
       "320  2efeea2e77a59e8f   [210.27, 127.41, 42.01, 49.73]                  9   \n",
       "339  b47636280fde8aee   [691.76, 627.63, 17.56, 14.87]                 10   \n",
       "362  03031fa84a2806a3  [594.47, 89.11, 105.86, 130.54]                  L   \n",
       "\n",
       "                                                points      area  img_num  \\\n",
       "9    [199.05, 0.7, 194.75, 326.86, 166.08, 327.58, ...  10777.23        9   \n",
       "151  [572.88, 204.36, 661.13, 213.72, 657.18, 234.6...   2757.91      151   \n",
       "320  [210.27, 128.27, 248.85, 127.41, 252.28, 175.4...   2089.16      320   \n",
       "339  [692.25, 627.63, 708.83, 627.87, 709.32, 642.5...    261.12      339   \n",
       "362  [594.47, 98.67, 659.34, 89.11, 700.33, 203.73,...  13818.96      362   \n",
       "\n",
       "     width  height             file_name  \n",
       "9      357    1024  a183473b9b4bc8f6.jpg  \n",
       "151   1024     683  42113d4ea98235b5.jpg  \n",
       "320    480    1024  2efeea2e77a59e8f.jpg  \n",
       "339    764    1024  b47636280fde8aee.jpg  \n",
       "362    891    1024  03031fa84a2806a3.jpg  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener los IDs de las imágenes en cada conjunto\n",
    "samp_imgs_train_ids = [os.path.basename(f) for f in train_samp]\n",
    "samp_imgs_val_ids = [os.path.basename(f) for f in val_samp]\n",
    "samp_imgs_test_ids = [os.path.basename(f) for f in test_samp]\n",
    "\n",
    "img_info_train = img_info_sampled[img_info_sampled['file_name'].isin(samp_imgs_train_ids)]\n",
    "img_info_val = img_info_sampled[img_info_sampled['file_name'].isin(samp_imgs_val_ids)]\n",
    "img_info_test = img_info_sampled[img_info_sampled['file_name'].isin(samp_imgs_test_ids)]\n",
    "\n",
    "print(\"Cantidad de registros en img_info para train:\", len(img_info_train))\n",
    "print(\"Cantidad de registros en img_info para val:\", len(img_info_val))\n",
    "print(\"Cantidad de registros en img_info para test:\", len(img_info_test))\n",
    "\n",
    "img_info_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8756b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (512, 512)\n",
    "\n",
    "def create_dataset(df, subset_name):\n",
    "    \"\"\"\n",
    "    Crea un tf.data.Dataset para un subconjunto (train, val o test)\n",
    "    a partir del DataFrame correspondiente y su carpeta de imágenes.\n",
    "    \"\"\"\n",
    "    images_path = os.path.join(path, 'train_val_images', f'{subset_name}')\n",
    "\n",
    "    # Extraer columnas necesarias\n",
    "    file_names = df['file_name'].values\n",
    "    texts = df['utf8_string'].values\n",
    "\n",
    "    # Crear dataset base\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_names, texts))\n",
    "\n",
    "    # Definir función de carga\n",
    "    def load_image_and_text(file_name, text):\n",
    "        image_path = tf.strings.join([images_path, '/', file_name])\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, IMG_SIZE)\n",
    "        image = image / 255.0\n",
    "        return image, text\n",
    "\n",
    "    # Mapear\n",
    "    dataset = dataset.map(load_image_and_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89ad4abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 269\n",
      "Val samples: 90\n",
      "Test samples: 87\n"
     ]
    }
   ],
   "source": [
    "# Crear datasets para cada conjunto\n",
    "train_dataset = create_dataset(img_info_train, \"train\")\n",
    "val_dataset = create_dataset(img_info_val, \"val\")\n",
    "test_dataset = create_dataset(img_info_test, \"test\")\n",
    "\n",
    "print(\"Train samples:\", len(img_info_train))\n",
    "print(\"Val samples:\", len(img_info_val))\n",
    "print(\"Test samples:\", len(img_info_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e9334",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91be8f",
   "metadata": {},
   "source": [
    "https://huggingface.co/facebook/detr-resnet-50\n",
    "\n",
    "https://huggingface.co/miguelcarv/resnet-50-text-detector\n",
    "\n",
    "https://huggingface.co/microsoft/trocr-base-printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7af87ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Crear processor y modelo  \n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d8ed8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_trocr(image, text):\n",
    "    # El processor espera una imagen tipo PIL, así que convertimos el tensor a numpy → PIL\n",
    "    image_np = tf.cast(image * 255, tf.uint8).numpy()\n",
    "    image_pil = Image.fromarray(image_np)\n",
    "    \n",
    "    # Convertimos texto → ids\n",
    "    labels = processor.tokenizer(\n",
    "        text.numpy().decode(\"utf-8\"),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).input_ids\n",
    "\n",
    "    # Procesar imagen con el processor (resize, normalización, etc.)\n",
    "    pixel_values = processor(images=image_pil, return_tensors=\"tf\").pixel_values[0]\n",
    "    return pixel_values, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8410c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_trocr(image, text):\n",
    "    pixel_values, labels = tf.py_function(\n",
    "        func=preprocess_for_trocr,\n",
    "        inp=[image, text],\n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    pixel_values.set_shape((3, 384, 384))  # tamaño fijo esperado por ViT base\n",
    "    labels.set_shape((64,))  # longitud máxima del texto\n",
    "    return {\"pixel_values\": pixel_values}, {\"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "660a5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trocr = train_dataset.map(map_to_trocr, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_trocr = val_dataset.map(map_to_trocr, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_trocr = test_dataset.map(map_to_trocr, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_trocr = train_trocr.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "val_trocr = val_trocr.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "test_trocr = test_trocr.batch(4).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d251d0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compile() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      5\u001b[39m steps_per_epoch = \u001b[32m269\u001b[39m\n\u001b[32m      6\u001b[39m optimizer, schedule = create_optimizer(\n\u001b[32m      7\u001b[39m     init_lr=\u001b[32m5e-5\u001b[39m,\n\u001b[32m      8\u001b[39m     num_train_steps=steps_per_epoch * num_epochs,\n\u001b[32m      9\u001b[39m     num_warmup_steps=\u001b[32m0\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2994\u001b[39m, in \u001b[36mModule.compile\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   2986\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2987\u001b[39m \u001b[33;03m    Compile this Module's forward using :func:`torch.compile`.\u001b[39;00m\n\u001b[32m   2988\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2992\u001b[39m \u001b[33;03m    See :func:`torch.compile` for details on the arguments for this function.\u001b[39;00m\n\u001b[32m   2993\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m     \u001b[38;5;28mself\u001b[39m._compiled_call_impl = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: compile() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 4\n",
    "num_epochs = 3\n",
    "steps_per_epoch = 269\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_train_steps=steps_per_epoch * num_epochs,\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "\n",
    "model.compile(optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_trocr,\n",
    "    validation_data=val_trocr,\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./trocr_finetuned_text\")\n",
    "processor.save_pretrained(\"./trocr_finetuned_text\")\n",
    "\n",
    "# Probar\n",
    "from PIL import Image\n",
    "image = Image.open(sample_path + '/fa10d2abe99d5af1.jpg').convert(\"RGB\")\n",
    "inputs = processor(images=image, return_tensors=\"tf\").pixel_values\n",
    "generated_ids = model.generate(inputs)\n",
    "text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25ab05a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m: pixel_values, \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: labels}\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Supongamos que ya tienes DataFrames train_df y val_df\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_dataset = Dataset.from_pandas(\u001b[43mtrain_df\u001b[49m).map(preprocess_batch, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m val_dataset = Dataset.from_pandas(val_df).map(preprocess_batch, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# --- 3. Entrenamiento ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Cargar modelo y processor ---\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\").to(\"cuda\")\n",
    "\n",
    "# --- 2. Crear dataset compatible ---\n",
    "def preprocess_batch(examples):\n",
    "    images = [Image.open(img_path).convert(\"RGB\") for img_path in examples[\"file_name\"]]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    labels = processor.tokenizer(\n",
    "        examples[\"utf8_string\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Supongamos que ya tienes DataFrames train_df y val_df\n",
    "train_dataset = Dataset.from_pandas(train_df).map(preprocess_batch, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(preprocess_batch, batched=True)\n",
    "\n",
    "# --- 3. Entrenamiento ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trocr-finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad3e9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 1\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::MapAndBatch::ParallelMapV2: NewRandomAccessFile failed to Create/Open: C:\\Users\\andre\\.cache\\kagglehub\\datasets\\robikscube\\textocr-text-extraction-from-images-dataset\\versions\\2\\train_val_images\\train/a183473b9b4bc8f6.jpg : El sistema no puede encontrar la ruta especificada.\r\n; No such process\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔹 Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_trocr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGradientTape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:826\u001b[39m, in \u001b[36mOwnedIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    825\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    827\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m errors.OutOfRangeError:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:776\u001b[39m, in \u001b[36mOwnedIterator._next_internal\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context.execution_mode(context.SYNC):\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m   ret = \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    782\u001b[39m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._element_spec._from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3086\u001b[39m, in \u001b[36miterator_get_next\u001b[39m\u001b[34m(iterator, output_types, output_shapes, name)\u001b[39m\n\u001b[32m   3084\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3085\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3086\u001b[39m   \u001b[43m_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   3088\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6027\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6025\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6026\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6027\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::MapAndBatch::ParallelMapV2: NewRandomAccessFile failed to Create/Open: C:\\Users\\andre\\.cache\\kagglehub\\datasets\\robikscube\\textocr-text-extraction-from-images-dataset\\versions\\2\\train_val_images\\train/a183473b9b4bc8f6.jpg : El sistema no puede encontrar la ruta especificada.\r\n; No such process\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Cargar modelo y processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")  # carga desde PyTorch\n",
    "\n",
    "# Forzar el uso de GPU\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n",
    "# Función para procesar las imágenes y textos\n",
    "def preprocess_for_trocr(image, text):\n",
    "    image_np = tf.cast(image * 255, tf.uint8).numpy()\n",
    "    image_pil = Image.fromarray(image_np)\n",
    "    pixel_values = processor(images=image_pil, return_tensors=\"tf\").pixel_values[0]\n",
    "    labels = processor.tokenizer(text.numpy().decode(\"utf-8\"), max_length=64, padding=\"max_length\", truncation=True).input_ids\n",
    "    return pixel_values, labels\n",
    "\n",
    "def map_to_trocr(image, text):\n",
    "    pixel_values, labels = tf.py_function(\n",
    "        func=preprocess_for_trocr,\n",
    "        inp=[image, text],\n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    pixel_values.set_shape((3, 384, 384))\n",
    "    labels.set_shape((64,))\n",
    "    return {\"pixel_values\": pixel_values}, {\"labels\": labels}\n",
    "\n",
    "# Prepara datasets\n",
    "train_trocr = train_dataset.map(map_to_trocr, num_parallel_calls=tf.data.AUTOTUNE).batch(4)\n",
    "val_trocr = val_dataset.map(map_to_trocr, num_parallel_calls=tf.data.AUTOTUNE).batch(4)\n",
    "\n",
    "# Optimizador\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "# Entrenamiento manual\n",
    "with tf.device(device):\n",
    "    for epoch in range(3):\n",
    "        print(f\"\\n🔹 Epoch {epoch+1}\")\n",
    "        for step, (x, y) in enumerate(train_trocr):\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(**x, labels=y[\"labels\"], return_dict=True)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step}: loss = {loss.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4286f9fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.trocr.configuration_trocr.TrOCRConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Cargar el modelo y el processor\u001b[39;00m\n\u001b[32m      6\u001b[39m processor = TrOCRProcessor.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mmicrosoft/trocr-base-printed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mTFAutoModelForVision2Seq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmicrosoft/trocr-base-printed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Compilar modelo\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_optimizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:2929\u001b[39m, in \u001b[36mTFPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   2926\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mload_weight_prefix\u001b[39m\u001b[33m\"\u001b[39m] = load_weight_prefix + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2928\u001b[39m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2931\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tf_to_pt_weight_rename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mtf_to_pt_weight_rename\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2932\u001b[39m     \u001b[38;5;66;03m# TODO Matt: This is a temporary workaround to allow weight renaming, but requires a method\u001b[39;00m\n\u001b[32m   2933\u001b[39m     \u001b[38;5;66;03m#            to be defined for each class that requires a rename. We can probably just have a class-level\u001b[39;00m\n\u001b[32m   2934\u001b[39m     \u001b[38;5;66;03m#            dict and a single top-level method or something and cut down a lot of boilerplate code\u001b[39;00m\n\u001b[32m   2935\u001b[39m     tf_to_pt_weight_rename = model.tf_to_pt_weight_rename\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_tf_vision_encoder_decoder.py:217\u001b[39m, in \u001b[36mTFVisionEncoderDecoderModel.__init__\u001b[39m\u001b[34m(self, config, encoder, decoder)\u001b[39m\n\u001b[32m    214\u001b[39m     encoder = TFAutoModel.from_config(config.encoder, name=\u001b[33m\"\u001b[39m\u001b[33mencoder\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     decoder = \u001b[43mTFAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28mself\u001b[39m.encoder = encoder\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.decoder = decoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\text-extraction-from-images\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:458\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_config\u001b[39m\u001b[34m(cls, config, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class._from_config(config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    459\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers.models.trocr.configuration_trocr.TrOCRConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForVision2Seq, TrOCRProcessor\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar el modelo y el processor\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
    "model = TFAutoModelForVision2Seq.from_pretrained('microsoft/trocr-base-printed')\n",
    "\n",
    "# Compilar modelo\n",
    "from transformers import create_optimizer\n",
    "batch_size = 4\n",
    "num_epochs = 3\n",
    "steps_per_epoch = len(train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_train_steps=steps_per_epoch * num_epochs,\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Entrenamiento manual con train_on_batch\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n🔹 Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in tqdm(train_trocr):\n",
    "        loss = model.train_on_batch(batch[0], batch[1])\n",
    "    print(f\"Loss epoch {epoch+1}: {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
